{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corporate-navigation",
   "metadata": {},
   "source": [
    "# US Financial Health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-politics",
   "metadata": {},
   "source": [
    "In this project, we’ll be importing various types of financial data to try and determine the financial health and volatility of the US between 1999 and 2019.\n",
    "We’ll use the techniques we’ve learned for importing financial data, to import stock and commodity pricing data from csv files and the FRED API. Then grab GDP and goods and services export data from the World Bank API.\n",
    "Finally, we’ll find the log returns of the imported data, and use that to determine the volatility of the data over the 20 year period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-coverage",
   "metadata": {},
   "source": [
    " -- In the workspace there are two csv files with historical commodity data for gold and crude oil.\n",
    "This is the commodity data we’ll be importing and operating on.\n",
    "In order to import csv files, we’ll need the pandas library imported into our program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distributed-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-chick",
   "metadata": {},
   "source": [
    "-- Now that pandas is imported, use its `read_csv` function to import data from the gold_prices.csv file into a variable called `gold_prices` and from crude_oil_prices.csv file into a varible `crude_oil_prices` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handy-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_prices = pd.read_csv('gold_prices.csv')\n",
    "crude_oil_prices = pd.read_csv('crude_oil_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tired-study",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Gold_Price\n",
      "0     2019-08-30     1528.40\n",
      "1     2019-08-29     1540.20\n",
      "2     2019-08-28     1537.15\n",
      "3     2019-08-27     1532.95\n",
      "4     2019-08-26     1503.80\n",
      "...          ...         ...\n",
      "5386  1999-01-07      289.95\n",
      "5387  1999-01-06      287.65\n",
      "5388  1999-01-05      287.15\n",
      "5389  1999-01-04      287.15\n",
      "5390  1999-01-01      287.80\n",
      "\n",
      "[5391 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(gold_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "certified-median",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Date  Crude_Oil_Price\n",
      "0     Sep 11, 2018            69.25\n",
      "1     Sep 10, 2018            67.54\n",
      "2     Sep 07, 2018            67.75\n",
      "3     Sep 06, 2018            67.77\n",
      "4     Sep 05, 2018            68.72\n",
      "...            ...              ...\n",
      "4995  Jan 08, 1999            13.07\n",
      "4996  Jan 07, 1999            13.09\n",
      "4997  Jan 06, 1999            12.80\n",
      "4998  Jan 05, 1999            11.99\n",
      "4999  Jan 04, 1999            12.34\n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(crude_oil_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-reset",
   "metadata": {},
   "source": [
    "-- Pandas datareader is able to import stock pricing data from the FRED API using the pandas_datareader.data library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "specified-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-group",
   "metadata": {},
   "source": [
    "-- Since we only want data between 1999 and 2019, we’ll also want to create some start and end variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mediterranean-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(1999, 1 ,1)\n",
    "end = datetime(2019, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-quebec",
   "metadata": {},
   "source": [
    "--We can use the `web.DataReader` function to get historical prices for the NASDAQ 100 from the FRED API.\n",
    "`web.DataReader` takes 4 arguments:\n",
    "* Data id code (`NASDAQ100`)\n",
    "* The name of the API we want to call (`fred`)\n",
    "* Start and end date times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "invisible-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "nasdaq_data = web.DataReader('NASDAQ100', 'fred', start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "arabic-replacement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            NASDAQ100\n",
      "DATE                 \n",
      "1999-01-01        NaN\n",
      "1999-01-04   1854.390\n",
      "1999-01-05   1903.000\n",
      "1999-01-06   1963.950\n",
      "1999-01-07   1966.350\n",
      "...               ...\n",
      "2018-12-26   6262.766\n",
      "2018-12-27   6288.301\n",
      "2018-12-28   6285.266\n",
      "2018-12-31   6329.965\n",
      "2019-01-01        NaN\n",
      "\n",
      "[5218 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(nasdaq_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-consumer",
   "metadata": {},
   "source": [
    "-- The FRED API also stores data from the S&P 500 Index. Let’s import that as well.Call web.DataReader just like in the previous step, except change the data id code from NASDAQ100 to SP500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "extensive-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "sap_data = web.DataReader('SP500', 'fred', start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "political-summit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              SP500\n",
      "DATE               \n",
      "2011-02-03  1307.10\n",
      "2011-02-04  1310.87\n",
      "2011-02-07  1319.05\n",
      "2011-02-08  1324.57\n",
      "2011-02-09  1320.88\n",
      "...             ...\n",
      "2018-12-26  2467.70\n",
      "2018-12-27  2488.83\n",
      "2018-12-28  2485.74\n",
      "2018-12-31  2506.85\n",
      "2019-01-01      NaN\n",
      "\n",
      "[2064 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sap_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-mitchell",
   "metadata": {},
   "source": [
    "-- In addition to stock and commodity prices, we also want to import more high level economic data like GDP and the total value of goods and services exported in a given year.\n",
    "Luckily for us, the World Bank API tracks exactly these things.\n",
    "First things first, let’s import the World Bank sub-module form pandas datareader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "express-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.wb as wb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-mambo",
   "metadata": {},
   "source": [
    "-- We can use the wb.download function to get GDP data from the World Bank API.\n",
    "wb.download takes 4 arguments:\n",
    "* A data indicator (NY.GDP.MKTP.CD)\n",
    "* list of countries to get data for\n",
    "* Start and end datetimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "noticed-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_data = wb.download(indicator='NY.GDP.MKTP.CD',\n",
    "                       country=['US'],\n",
    "                       start = start,\n",
    "                       end = end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "piano-mauritius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    NY.GDP.MKTP.CD\n",
      "country       year                \n",
      "United States 2019  21433226000000\n",
      "              2018  20580159776000\n",
      "              2017  19519353692000\n",
      "              2016  18714960538000\n",
      "              2015  18224704440000\n",
      "              2014  17527163695000\n",
      "              2013  16784849196000\n",
      "              2012  16197007349000\n",
      "              2011  15542581104000\n",
      "              2010  14992052727000\n",
      "              2009  14448933025000\n",
      "              2008  14712844084000\n",
      "              2007  14451858656000\n",
      "              2006  13814611414000\n",
      "              2005  13036640230000\n",
      "              2004  12213729147000\n",
      "              2003  11458243878000\n",
      "              2002  10936419054000\n",
      "              2001  10581821399000\n",
      "              2000  10252345464000\n",
      "              1999   9630664202000\n"
     ]
    }
   ],
   "source": [
    "print(gdp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-register",
   "metadata": {},
   "source": [
    "-- The World Bank API also stores data about the value of goods and services exported in a given year. Let’s import that as well.\n",
    "Call wb.download just like in the previous step, except change the indicator from NY.GDP.MKTP.CD to NE.EXP.GNFS.CN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "relevant-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data = wb.download(indicator='NE.EXP.GNFS.CN',\n",
    "                       country=['US'],\n",
    "                       start = start,\n",
    "                       end = end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "regulation-aside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    NE.EXP.GNFS.CN\n",
      "country       year                \n",
      "United States 2019   2514751000000\n",
      "              2018   2528704000000\n",
      "              2017   2374560000000\n",
      "              2016   2227174000000\n",
      "              2015   2265862000000\n",
      "              2014   2371704000000\n",
      "              2013   2273428000000\n",
      "              2012   2191280000000\n",
      "              2011   2102995000000\n",
      "              2010   1846280000000\n",
      "              2009   1581996000000\n",
      "              2008   1837055000000\n",
      "              2007   1660853000000\n",
      "              2006   1472613000000\n",
      "              2005   1305225000000\n",
      "              2004   1177631000000\n",
      "              2003   1036177000000\n",
      "              2002    998741000000\n",
      "              2001   1024636000000\n",
      "              2000   1096255000000\n",
      "              1999    992778000000\n"
     ]
    }
   ],
   "source": [
    "print(export_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-roller",
   "metadata": {},
   "source": [
    "-- At this point, we’ve imported all the data we need. But it’s all stored as either daily or yearly dollar amounts.\n",
    "Pricing data is useful, but in this case, since we want to compare each data set, it would be even better if instead of daily/yearly pricing, we had information on the log returns from the daily/yearly prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-riding",
   "metadata": {},
   "source": [
    "-- In our case we want to run this equation for each day/year of pricing data in our imported DataFrame Series (A Series is a single column in a DataFrame).\n",
    "The pandas shift function can be used to divide each current price by its previous price in the Series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "compressed-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_return(prices):\n",
    "  return np.log(prices / prices.shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-sympathy",
   "metadata": {},
   "source": [
    "-- And we can use numpy’s natural log function to get the log return for each entry in the new Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spoken-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-sheriff",
   "metadata": {},
   "source": [
    "Let’s use our new log_return function to calculate the log return of the gold_prices DataFrame we imported. After that create log return variables for each additional dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dense-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_returns = log_return(gold_prices['Gold_Price'])\n",
    "crude_oil_returns = log_return(crude_oil_prices['Crude_Oil_Price'])\n",
    "nasdaq_returns = log_return(nasdaq_data['NASDAQ100'])\n",
    "sap_returns = log_return(sap_data['SP500'])\n",
    "gdp_returns = log_return(gdp_data['NY.GDP.MKTP.CD'])\n",
    "export_returns = log_return(export_data['NE.EXP.GNFS.CN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-engine",
   "metadata": {},
   "source": [
    "-- We’re now ready to compare the volatility of each type of data.\n",
    "* Variance, in the context of financial data, tells us how volatile an investment is. Use Panda’s `var()` function to calculate the variance of the commodities, stocks and World Bank data returns, and print the results.\n",
    "* The results can be interpreted in a number of ways, but generally, the higher the variance the more volatile the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "double-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Variance : 0.00011375928671558508\n",
      "Oil Variance : 0.0005563207795629881\n",
      "Nasdaq Variance : 0.0003178379833057229\n",
      "SAP Variance : 8.45669303086758e-05\n",
      "GDP Variance : 0.000342043019210802\n",
      "Export Variance : 0.006201903531105136\n"
     ]
    }
   ],
   "source": [
    "print('Gold Variance :', gold_returns.var())\n",
    "print('Oil Variance :', crude_oil_returns.var())\n",
    "print('Nasdaq Variance :', nasdaq_returns.var())\n",
    "print('SAP Variance :', sap_returns.var())\n",
    "print('GDP Variance :', gdp_returns.var())\n",
    "print('Export Variance :', export_returns.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-tissue",
   "metadata": {},
   "source": [
    "The S&P 500, a collection of 500 large companies listed on stock exchanges in the United States, has the smallest variance, and thus is the least volatile. Given that the S&P 500 index is a weighted measurement of many stocks across a variety of industries, it is seen as a safer, diversified investment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-familiar",
   "metadata": {},
   "source": [
    "Gold, notorious for being a stable investment has the second smallest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-moscow",
   "metadata": {},
   "source": [
    "Crude oil is the most volatile, which makes sense as gas prices are often unpredictable, especially in the last 20 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-crowd",
   "metadata": {},
   "source": [
    "The stocks are interesting. The NASDAQ 100 is more volatile than the S&P 500, which, when you think about it makes sense, as the S&P 500 is far more diversified and tracks more of the market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-effort",
   "metadata": {},
   "source": [
    "Then finally we have GDP and exports.\n",
    "* Exports are very volatile, which could have to do with industries moving overseas in the last 20 years, and global competition for the production of goods.\n",
    "* GDP is actually fairly similar to the NASDAQ 100 in terms of volatility, which is perhaps an interesting correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-history",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
